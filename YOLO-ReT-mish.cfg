[net]
# Training
batch=16
subdivisions=8
height=224
width=224
channels=3
momentum=0.949
decay=0.0005
#max_crop=320

#cutmix=1
mosaic=1
label_smooth_eps=0.1

burn_in=1000
learning_rate=0.001
policy=step
step=10000
scale=0.96
max_batches=5080
momentum=0.9
decay=0.00005

angle=7
hue=.1
saturation=.75
exposure=.75
aspect=.75


### CONV1 - 1 (1)
# conv1
[convolutional]
filters=40	#32
size=3
pad=1
stride=2
batch_normalize=1
activation=mish


### CONV2 - MBConv1 - 1 (2)
# conv2_1_expand
[convolutional]
filters=40	#32
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv2_1_dwise
[convolutional]
groups=40	#32
filters=40	#32
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv2_1_linear
[convolutional]
filters=16	#16
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV2 - MBConv1 - 2 (2)
# conv2_1_expand
[convolutional]
filters=40	#32
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv2_1_dwise
[convolutional]
groups=40	#32
filters=40	#32
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv2_1_linear
[convolutional]
filters=16	#16
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV3 - MBConv6 - 1 (3)
# dropout only before residual connection
[dropout]
probability=.3

# block_3_1
[shortcut]
from=-5
activation=linear

# conv2_2_expand
[convolutional]
filters=112	#96
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv2_2_dwise
[convolutional]
groups=112	#96
filters=112	#96
size=3
pad=1
stride=2
batch_normalize=1
activation=mish

# conv2_2_linear
[convolutional]
filters=32	#24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV3 - MBConv6 - 2 (3)
# conv3_1_expand
[convolutional]
filters=176	#144
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv3_1_dwise
[convolutional]
groups=176	#144
filters=176	#144
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv3_1_linear
[convolutional]
filters=32	#24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV3 - MBConv6 - 3 (3)
# dropout only before residual connection
[dropout]
probability=.3

# block_3_1
[shortcut]
from=-5
activation=linear

# conv3_1_expand
[convolutional]
filters=176	#144
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv3_1_dwise
[convolutional]
groups=176	#144
filters=176	#144
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv3_1_linear
[convolutional]
filters=32	#24
size=1
stride=1
pad=0
batch_normalize=1
activation=linear



### CONV4 - MBConv6 - 1 (3)
# dropout only before residual connection
[dropout]
probability=.3

# block_3_1
[shortcut]
from=-5
activation=linear

# conv_3_2_expand
[convolutional]
filters=176	#144
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_3_2_dwise
[convolutional]
groups=176	#144
filters=176	#144
size=5
pad=1
stride=2
batch_normalize=1
activation=mish

# conv_3_2_linear
[convolutional]
filters=48	#40
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV4 - MBConv6 - 2 (3)
# conv_4_1_expand
[convolutional]
filters=232	#192
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_1_dwise
[convolutional]
groups=232	#192
filters=232	#192
size=5
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_1_linear
[convolutional]
filters=48	#40
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV4 - MBConv6 - 3 (3)
# dropout only before residual connection
[dropout]
probability=.3

# block_4_2
[shortcut]
from=-5
activation=linear

# conv_4_1_expand
[convolutional]
filters=232	#192
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_1_dwise
[convolutional]
groups=232	#192
filters=232	#192
size=5
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_1_linear
[convolutional]
filters=48	#40
size=1
stride=1
pad=0
batch_normalize=1
activation=linear




### CONV5 - MBConv6 - 1 (5)
# dropout only before residual connection
[dropout]
probability=.3

# block_4_2
[shortcut]
from=-5
activation=linear

# conv_4_3_expand
[convolutional]
filters=232	#192
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_3_dwise
[convolutional]
groups=232	#192
filters=232	#192
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_3_linear
[convolutional]
filters=96	#80
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV5 - MBConv6 - 2 (5)
# conv_4_4_expand
[convolutional]
filters=464	#384
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_4_dwise
[convolutional]
groups=464	#384
filters=464	#384
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_4_linear
[convolutional]
filters=96	#80
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV5 - MBConv6 - 3 (5)
# dropout only before residual connection
[dropout]
probability=.3

# block_4_4
[shortcut]
from=-5
activation=linear

# conv_4_5_expand
[convolutional]
filters=464	#384
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_5_dwise
[convolutional]
groups=464	#384
filters=464	#384
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_5_linear
[convolutional]
filters=96	#80
size=1
stride=1
pad=0
batch_normalize=1
activation=linear


### CONV5 - MBConv6 - 4 (5)
# dropout only before residual connection
[dropout]
probability=.3

# block_4_4
[shortcut]
from=-5
activation=linear

# conv_4_5_expand
[convolutional]
filters=464	#384
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_5_dwise
[convolutional]
groups=464	#384
filters=464	#384
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_5_linear
[convolutional]
filters=96	#80
size=1
stride=1
pad=0
batch_normalize=1
activation=linear



### CONV5 - MBConv6 - 5 (5)
# dropout only before residual connection
[dropout]
probability=.3

# block_4_4
[shortcut]
from=-5
activation=linear

# conv_4_5_expand
[convolutional]
filters=464	#384
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_5_dwise
[convolutional]
groups=464	#384
filters=464	#384
size=3
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_4_5_linear
[convolutional]
filters=96	#80
size=1
stride=1
pad=0
batch_normalize=1
activation=linear



### CONV6 - MBConv6 - 1 (5)
# dropout only before residual connection
[dropout]
probability=.3

# block_4_6
[shortcut]
from=-5
activation=linear

# conv_4_7_expand
[convolutional]
filters=464	#384
size=1
stride=1
pad=0
batch_normalize=1
activation=mish

# conv_4_7_dwise
[convolutional]
groups=464	#384
filters=464	#384
size=5
pad=1
stride=2
batch_normalize=1
activation=mish
stopbackward = 800

#from first downsample 144x144x40
[route]
layers = 0

[maxpool]
size = 1
stride = 2

[maxpool]
size = 1
stride = 2

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=40
activation=leaky
#END from first downsample

#from second downsample 72x72x112
[route]
layers = 10

[maxpool]
size = 1
stride = 2

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=112
activation=leaky
#END from second downsample

#from third downsample 36x36x176
[route]
layers = 23

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=176
activation=leaky
#END from third downsample

#from fourth downsample 18x18x464
[route]
layers = 59

[upsample]
stride = 2

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=464
activation=leaky
#END from fourth downsample

#weighted sum
[shortcut]
from = -1,-4, -6, -9
weights_type = per_feature
weights_normalization = relu
activation = linear

#MBconv block with 5x5 kernel
# conv_6_2_expand
[convolutional]
filters=464	#960
size=5
stride=1
pad=1
batch_normalize=1
activation=mish

# conv_6_2_dwise
[convolutional]
groups=464	#960
filters=464	#960
size=5
stride=1
pad=1
batch_normalize=1
activation=mish

#END MBconv block with 5x5 kernel

#concat from second downsample
[upsample]
stride = 2

[route]
layers = -1,10

#concat from third downsample
[route]
layers = -3,23

#concat from fourth downsample
[route]
layers = -4

[maxpool]
size = 1
stride = 2

[route]
layers = -1, 59

#BEGIN SPP SECTION
[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=464
activation=leaky

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

### SPP ###
[maxpool]
stride=1
size=5

[route]
layers=-2

[maxpool]
stride=1
size=9

[route]
layers=-4

[maxpool]
stride=1
size=13

[route]
layers=-1,-3,-5,-6
### End SPP ###

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=464
activation=leaky

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

#END SPP SECTION

#BEGIN PANet
[convolutional]
batch_normalize=1
filters=132
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride = 2

#concat from third downsample
[route]
layers = 23

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -1,-3

#CBLx5

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=434
activation=leaky

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=434
activation=leaky

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

#END CBLx5

[convolutional]
batch_normalize=1
filters=132
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride = 2

#concat from second downsample
[route]
layers = 10

[convolutional]
batch_normalize=1
filters=88
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -1,-3

#CBLx5
[convolutional]
batch_normalize=1
filters=132
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=232
activation=leaky

[convolutional]
batch_normalize=1
filters=132
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=232
activation=leaky

[convolutional]
batch_normalize=1
filters=132
size=1
stride=1
pad=1
activation=leaky

#END CBLx5

#YOLOHead leftmost

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=232
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear


[yolo]
mask = 0,1,2
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
scale_x_y = 1.2
iou_thresh=0.213
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=giou
nms_kind=greedynms
beta_nms=0.6
max_delta=5

#END leftmost YOLOHead

#Downsample from first cblx5 going towards middle yolohead
[route]
layers = -4

[convolutional]
batch_normalize=1
size=1
stride=2
pad=1
filters=132
activation=leaky

[route]
layers = -1, -16

#Third CBLx5
[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=464
activation=leaky

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=464
activation=leaky

[convolutional]
batch_normalize=1
filters=232
size=1
stride=1
pad=1
activation=leaky
#END third cblx5

#YOLOHead middle
[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=464
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear


[yolo]
mask = 3,4,5
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
scale_x_y = 1.1
iou_thresh=0.213
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=giou
nms_kind=greedynms
beta_nms=0.6
max_delta=5
#END Middle yolohead

#Downsample from third cblx5 going towards rightmost yolohead
[route]
layers = -4

[convolutional]
batch_normalize=1
size=1
stride=2
pad=1
filters=464
activation=leaky

[route]
layers = -1, -37

#Fourth CBLx5
[convolutional]
batch_normalize=1
filters=464
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=688
activation=leaky

[convolutional]
batch_normalize=1
filters=464
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=688
activation=leaky

[convolutional]
batch_normalize=1
filters=464
size=1
stride=1
pad=1
activation=leaky
#END fourth cblx5

#YOLOhead rightmost
[convolutional]
batch_normalize=1
size=1
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=18
activation=linear


[yolo]
mask = 6,7,8
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=1
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
scale_x_y = 1.05
iou_thresh=0.213
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=giou
nms_kind=greedynms
beta_nms=0.6
max_delta=5
#END yolohead rightmost



